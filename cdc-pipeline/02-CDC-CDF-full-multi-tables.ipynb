{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13959b31-abb2-499e-b16c-c4d5f4ae4621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Full demo: Change Data Capture on multiple tables\n",
    "## Use-case: Synchronize all your ELT tables with your Lakehouse\n",
    "\n",
    "We previously saw how to synchronize a single table. However, real use-case typically includes multiple tables that we need to ingest and synch.\n",
    "\n",
    "These tables are stored on different folder having the following layout:\n",
    "\n",
    "<img width=\"1000px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/product/Delta-Lake-CDC-CDF/cdc-full.png\">\n",
    "\n",
    "**A note on Spark Declarative Pipelines**:<br/>\n",
    "*Spark Declarative Pipelines has been designed to simplify this process and handle concurrent execution properly, without having you to start multiple stream in parallel.*<br/>\n",
    "*We strongly advise to have a look at the SDP CDC demo to simplify such pipeline implementation: `dbdemos.instal('dlt-cdc')`*\n",
    "\n",
    "In this notebook, we'll see how this can be done using Python & standard streaming APIs (without SDP).\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=1444828305810485&notebook=%2F02-CDC-CDF-full-multi-tables&demo_name=cdc-pipeline&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fcdc-pipeline%2F02-CDC-CDF-full-multi-tables&version=1&user_hash=9eaf9fc8dee4c34ed24b2322b88a342ef579727ebb0f0e70e8a37fe40a0e2d5f\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64f166be-d62f-48d7-b6e0-468bcbbfb708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b1a90f0-e4ed-474f-80e8-a259d1f23180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Running the streams in parallel\n",
    "\n",
    "Each table will be save as a distinct table, using a distinct Spark Structured Streaming strem.\n",
    "\n",
    "To implement an efficient pipeline, we should process multiple streams at the same time. To do that, we'll use a ThreadPoolExecutor and start multiple thread, each of them processing and waiting for a stream.\n",
    "\n",
    "We're using Trigger Once to refresh all the tables once and then shutdown the cluster, typically every hour. For lower latencies we can keep the streams running (depending of the number of tables & cluster size), or keep the Trigger Once but loop forever.\n",
    "\n",
    "*Note that for a real workload the exact number of streams depends of the total number of tables, table sizes and cluster size. We can also use several clusters to split the load if required*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c669c96-0758-45e8-8961-c926f6b214e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Schema evolution\n",
    "\n",
    "By organizing the raw incoming cdc files with 1 folder by table, we can easily iterate over the folders and pickup any new tables without modification.\n",
    "\n",
    "Schema evolution will be handled my the Autoloader and Delta `mergeSchema` option at the bronze layer. Schema evolution for MERGE (Silver Layer) are supported using `spark.databricks.delta.schema.autoMerge.enabled`\n",
    "\n",
    "Using these options, we'll be able to capture new tables and table schema evolution without having to change our code.\n",
    "\n",
    "*Note: that autoloader will trigger an error in a stream if a schema change happens, and will automatically recover during the next run. See Autoloader demo for a complete example*\n",
    "\n",
    "*Note: another common pattern is to redirect all the CDC events to a single message queue (the table name being a message attribute), and then dispatch the message in different Silver Tables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35f6214a-3b2e-41f2-a20e-9e829e6eb40c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's explore our raw cdc data. We have 2 tables we want to sync (transactions and users)"
    }
   },
   "outputs": [],
   "source": [
    "base_folder = f\"{raw_data_location}/cdc\"\n",
    "display(dbutils.fs.ls(base_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77830d9b-209c-4089-beeb-f208016a4671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver and bronze transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a041c188-0827-4c6f-831a-907089519d64",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "let's reset all checkpoints"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(f\"{raw_data_location}/cdc_full\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6942aae1-9b85-44ce-9127-cc76c32dcf74",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bronze ingestion with autoloader"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Stream using the autoloader to ingest raw files and load them in a delta table\n",
    "def update_bronze_layer(path, bronze_table):\n",
    "  print(f\"ingesting RAW cdc data for {bronze_table} and building bronze layer...\")\n",
    "  (spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"csv\")\n",
    "          .option(\"cloudFiles.schemaLocation\", f\"{raw_data_location}/cdc_full/schemas/{bronze_table}\")\n",
    "          .option(\"cloudFiles.schemaHints\", \"id bigint, operation_date timestamp\")\n",
    "          .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "          .load(path)\n",
    "       .withColumn(\"file_name\", col(\"_metadata.file_path\"))\n",
    "       .writeStream\n",
    "          .option(\"checkpointLocation\", f\"{raw_data_location}/cdc_full/checkpoints/{bronze_table}\")\n",
    "          .option(\"mergeSchema\", \"true\")\n",
    "          #.trigger(processingTime='10 seconds')\n",
    "          .trigger(availableNow=True)\n",
    "          .table(bronze_table).awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceae74fc-b962-4a10-804c-0cbf0e4169c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Silver step: materialize tables with MERGE based on CDC events"
    }
   },
   "outputs": [],
   "source": [
    "#Stream incrementally loading new data from the bronze CDC table and merging them in the Silver table\n",
    "def update_silver_layer(bronze_table, silver_table):\n",
    "  print(f\"ingesting {bronze_table} update and materializing silver layer using a MERGE statement...\")\n",
    "  #First create the silver table if it doesn't exists:\n",
    "  if not spark.catalog.tableExists(silver_table):\n",
    "    print(f\"Table {silver_table} doesn't exist, creating it using the same schema as the bronze one...\")\n",
    "    spark.read.table(bronze_table).drop(\"operation\", \"operation_date\", \"_rescued_data\", \"file_name\").write.saveAsTable(silver_table)\n",
    "\n",
    "  #for each batch / incremental update from the raw cdc table, we'll run a MERGE on the silver table\n",
    "  def merge_stream(updates, i):\n",
    "    #First we need to deduplicate based on the id and take the most recent update\n",
    "    windowSpec = Window.partitionBy(\"id\").orderBy(col(\"operation_date\").desc())\n",
    "    #Select only the first value \n",
    "    #getting the latest change is still needed if the cdc contains multiple time the same id. We can rank over the id and get the most recent _commit_version\n",
    "    updates_deduplicated = updates.withColumn(\"rank\", row_number().over(windowSpec)).where(\"rank = 1\").drop(\"operation_date\", \"_rescued_data\", \"file_name\", \"rank\")\n",
    "    #Remove the \"operation\" field from the column to update in the silver table (we don't want the technical \"operation\" field to appear here)\n",
    "    columns_to_update = {c: f\"updates.{c}\"  for c in spark.read.table(silver_table).columns if c != \"operation\"}\n",
    "    #run the merge in the silver table directly\n",
    "    DeltaTable.forName(spark, silver_table).alias(\"target\") \\\n",
    "        .merge(updates_deduplicated.alias(\"updates\"), \"updates.id = target.id\") \\\n",
    "        .whenMatchedDelete(\"updates.operation = 'DELETE'\") \\\n",
    "        .whenMatchedUpdate(\"updates.operation != 'DELETE'\", set=columns_to_update) \\\n",
    "        .whenNotMatchedInsert(\"updates.operation != 'DELETE'\", values=columns_to_update) \\\n",
    "        .execute()\n",
    "    \n",
    "  (spark.readStream\n",
    "         .table(bronze_table)\n",
    "       .writeStream\n",
    "         .foreachBatch(merge_stream)\n",
    "         .option(\"checkpointLocation\", f\"{raw_data_location}/cdc_full/checkpoints/{silver_table}\")\n",
    "          #.trigger(processingTime='10 seconds')\n",
    "          .trigger(availableNow=True)\n",
    "          .start().awaitTermination())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7899eee3-4197-4aae-aea6-04b1bb7d7ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Starting all the streams\n",
    "\n",
    "We can now iterate over the folders to start the bronze & silver streams for each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec9cd819-d33a-4cac-a5da-0566c2902426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import deque\n",
    "from delta.tables import *\n",
    " \n",
    "def refresh_cdc_table(table):\n",
    "  try:\n",
    "    #update the bronze table\n",
    "    bronze_table = f'bronze_{table}'\n",
    "    update_bronze_layer(f\"{base_folder}/{table}\", bronze_table)\n",
    "\n",
    "    #then refresh the silver layer\n",
    "    silver_table = f'silver_{table}'\n",
    "    update_silver_layer(bronze_table, silver_table)\n",
    "  except Exception as e:\n",
    "    #prod workload should properly process errors\n",
    "    print(f\"couldn't properly process {bronze_table}\")\n",
    "    raise e\n",
    "  \n",
    "#Enable Schema evolution during merges (to capture new columns)  \n",
    "#spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "#iterate over all the tables folders\n",
    "tables = [table_path.name[:-1] for table_path in dbutils.fs.ls(base_folder)]\n",
    "#Let's start 3 CDC flow at the same time in 3 different thread to speed up ingestion\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "  deque(executor.map(refresh_cdc_table, tables))\n",
    "  print(f\"Database refreshed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a546d9-01a8-465a-8605-04d7abf2063e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from bronze_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9491fd6a-c476-4014-b2e1-028abf924813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from silver_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42272667-c8a2-4f7d-8a45-ad880dcfcfab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from silver_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ed3bb3-4b54-4ab0-9e7e-e67df707c7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## What's next\n",
    "\n",
    "All our silver tables are now materialized using the CDC events! We can then work extra transformation (gold layer) based on your business requirement.\n",
    "\n",
    "### Production readiness\n",
    "Error and exception in each stream should be properly captured. Multiple strategy exist: send a notification when a table has some error and continue processing the others, stop the entire job, define table \"priorities\" etc.\n",
    "\n",
    "### Spark Declarative Pipelines\n",
    "To simplify these operations & error handling, we strongly advise you to run your CDC pipelines on top of Spark Declarative Pipelines: `dbdemos.install('pipeline-bike')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce800ea-6a17-4ea1-8012-0d551def4ca9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Make sure we stop all actives streams"
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.stop_all_streams()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "02-CDC-CDF-full-multi-tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
