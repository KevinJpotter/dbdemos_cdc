{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62ebb7c0-6126-40e5-89f4-492bd1db40cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pytest fixtures for CDC Pipeline unit tests.\n",
    "These fixtures provide mock data and Spark session setup for testing.\n",
    "\"\"\"\n",
    "from dbx_test.fixtures import register_fixture, notebook_fixture\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, LongType, TimestampType, IntegerType\n",
    ")\n",
    "from datetime import datetime, timedelta\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def test_catalog(spark):\n",
    "    \"\"\"Get or create test catalog name.\"\"\"\n",
    "    return \"test_cdc_catalog\"\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def test_schema(spark):\n",
    "    \"\"\"Get or create test schema/database name.\"\"\"\n",
    "    return \"test_cdc_schema\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Schema Fixtures\n",
    "# ============================================================================\n",
    "\n",
    "@notebook_fixture\n",
    "def cdc_schema():\n",
    "    \"\"\"Schema for CDC input data (Bronze layer).\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"id\", LongType(), False),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"address\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"operation_date\", TimestampType(), True),\n",
    "        StructField(\"operation\", StringType(), True),\n",
    "        StructField(\"_rescued_data\", StringType(), True),\n",
    "        StructField(\"file_name\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def silver_schema():\n",
    "    \"\"\"Schema for Silver layer table.\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"id\", LongType(), False),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"address\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"operation\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def gold_schema():\n",
    "    \"\"\"Schema for Gold layer table.\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"id\", LongType(), False),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"address\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"gold_data\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def cdf_schema():\n",
    "    \"\"\"Schema for CDF (Change Data Feed) output.\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"id\", LongType(), False),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"address\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"operation\", StringType(), True),\n",
    "        StructField(\"_change_type\", StringType(), True),\n",
    "        StructField(\"_commit_version\", LongType(), True),\n",
    "        StructField(\"_commit_timestamp\", TimestampType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Sample Data Fixtures\n",
    "# ============================================================================\n",
    "\n",
    "@notebook_fixture\n",
    "def base_timestamp():\n",
    "    \"\"\"Base timestamp for test data.\"\"\"\n",
    "    return datetime(2024, 1, 1, 10, 0, 0)\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def sample_cdc_data(spark, cdc_schema, base_timestamp):\n",
    "    \"\"\"Sample CDC data with INSERT, UPDATE, DELETE operations.\"\"\"\n",
    "    data = [\n",
    "        (1, \"Alice\", \"123 Main St\", \"alice@email.com\", base_timestamp, \"INSERT\", None, \"file1.csv\"),\n",
    "        (2, \"Bob\", \"456 Oak Ave\", \"bob@email.com\", base_timestamp, \"INSERT\", None, \"file1.csv\"),\n",
    "        (3, \"Charlie\", \"789 Pine Rd\", \"charlie@email.com\", base_timestamp, \"INSERT\", None, \"file1.csv\"),\n",
    "    ]\n",
    "    return spark.createDataFrame(data, cdc_schema)\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def sample_cdc_with_updates(spark, cdc_schema, base_timestamp):\n",
    "    \"\"\"Sample CDC data including UPDATE operations.\"\"\"\n",
    "    data = [\n",
    "        (1, \"Alice\", \"123 Main St\", \"alice@email.com\", base_timestamp, \"INSERT\", None, \"file1.csv\"),\n",
    "        (2, \"Bob\", \"456 Oak Ave\", \"bob@email.com\", base_timestamp, \"INSERT\", None, \"file1.csv\"),\n",
    "        (1, \"Alice Updated\", \"999 New St\", \"alice.new@email.com\", \n",
    "         base_timestamp + timedelta(hours=1), \"UPDATE\", None, \"file2.csv\"),\n",
    "    ]\n",
    "    return spark.createDataFrame(data, cdc_schema)\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def sample_cdc_with_deletes(spark, cdc_schema, base_timestamp):\n",
    "    \"\"\"Sample CDC data including DELETE operations.\"\"\"\n",
    "    data = [\n",
    "        (1, \"Alice\", \"123 Main St\", \"alice@email.com\", base_timestamp, \"INSERT\", None, \"file1.csv\"),\n",
    "        (2, \"Bob\", \"456 Oak Ave\", \"bob@email.com\", base_timestamp, \"INSERT\", None, \"file1.csv\"),\n",
    "        (2, None, None, None, base_timestamp + timedelta(hours=1), \"DELETE\", None, \"file2.csv\"),\n",
    "    ]\n",
    "    return spark.createDataFrame(data, cdc_schema)\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def sample_cdc_with_duplicates(spark, cdc_schema, base_timestamp):\n",
    "    \"\"\"Sample CDC data with duplicate IDs requiring deduplication.\"\"\"\n",
    "    data = [\n",
    "        (1, \"Alice\", \"123 Main St\", \"alice@email.com\", base_timestamp, \"INSERT\", None, \"file1.csv\"),\n",
    "        (1, \"Alice V2\", \"456 New St\", \"alice2@email.com\", \n",
    "         base_timestamp + timedelta(hours=1), \"UPDATE\", None, \"file2.csv\"),\n",
    "        (1, \"Alice V3\", \"789 Final St\", \"alice3@email.com\", \n",
    "         base_timestamp + timedelta(hours=2), \"UPDATE\", None, \"file3.csv\"),\n",
    "    ]\n",
    "    return spark.createDataFrame(data, cdc_schema)\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def sample_silver_data(spark, silver_schema):\n",
    "    \"\"\"Sample Silver layer data.\"\"\"\n",
    "    data = [\n",
    "        (1, \"Alice\", \"123 Main St\", \"alice@email.com\", \"INSERT\"),\n",
    "        (2, \"Bob\", \"456 Oak Ave\", \"bob@email.com\", \"INSERT\"),\n",
    "        (3, \"Charlie\", \"789 Pine Rd\", \"charlie@email.com\", \"INSERT\"),\n",
    "    ]\n",
    "    return spark.createDataFrame(data, silver_schema)\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def sample_cdf_data(spark, base_timestamp):\n",
    "    \"\"\"Sample CDF (Change Data Feed) data for Gold layer tests.\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", LongType(), False),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"address\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"gold_data\", StringType(), True),\n",
    "        StructField(\"_change_type\", StringType(), True),\n",
    "        StructField(\"_commit_version\", LongType(), True),\n",
    "    ])\n",
    "    data = [\n",
    "        (1, \"Alice\", \"123 Main St\", \"alice@email.com\", \"Delta CDF is Awesome\", \"insert\", 1),\n",
    "        (2, \"Bob\", \"456 Oak Ave\", \"bob@email.com\", \"Delta CDF is Awesome\", \"insert\", 1),\n",
    "        (1, \"Alice\", \"123 Main St\", \"alice@email.com\", \"Delta CDF is Awesome\", \"update_preimage\", 2),\n",
    "        (1, \"Alice Updated\", \"999 New St\", \"alice.new@email.com\", \"Delta CDF is Awesome\", \"update_postimage\", 2),\n",
    "    ]\n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Table Management Fixtures\n",
    "# ============================================================================\n",
    "\n",
    "@notebook_fixture\n",
    "def temp_table_cleanup(spark):\n",
    "    \"\"\"\n",
    "    Fixture that yields table names and cleans them up after test.\n",
    "    Usage: tables = temp_table_cleanup; tables.append(\"my_temp_table\")\n",
    "    \"\"\"\n",
    "    tables_to_cleanup = []\n",
    "    yield tables_to_cleanup\n",
    "    for table in tables_to_cleanup:\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def create_test_table(spark, temp_table_cleanup):\n",
    "    \"\"\"\n",
    "    Factory fixture to create test Delta tables.\n",
    "    Returns a function that creates tables and registers them for cleanup.\n",
    "    \"\"\"\n",
    "    def _create_table(table_name, df, enable_cdf=False):\n",
    "        # Write DataFrame as Delta table\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "        \n",
    "        # Enable CDF if requested\n",
    "        if enable_cdf:\n",
    "            spark.sql(f\"ALTER TABLE {table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "        \n",
    "        temp_table_cleanup.append(table_name)\n",
    "        return table_name\n",
    "    \n",
    "    return _create_table\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Multi-Table Test Fixtures\n",
    "# ============================================================================\n",
    "\n",
    "@notebook_fixture\n",
    "def multi_table_config():\n",
    "    \"\"\"Configuration for multi-table CDC tests.\"\"\"\n",
    "    return {\n",
    "        \"users\": {\n",
    "            \"columns\": [\"id\", \"name\", \"email\", \"operation\", \"operation_date\"],\n",
    "            \"id_column\": \"id\"\n",
    "        },\n",
    "        \"transactions\": {\n",
    "            \"columns\": [\"id\", \"user_id\", \"amount\", \"operation\", \"operation_date\"],\n",
    "            \"id_column\": \"id\"\n",
    "        },\n",
    "        \"products\": {\n",
    "            \"columns\": [\"id\", \"name\", \"price\", \"operation\", \"operation_date\"],\n",
    "            \"id_column\": \"id\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def sample_users_cdc(spark, base_timestamp):\n",
    "    \"\"\"Sample users CDC data for multi-table tests.\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", LongType(), False),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"operation\", StringType(), True),\n",
    "        StructField(\"operation_date\", TimestampType(), True),\n",
    "        StructField(\"_rescued_data\", StringType(), True),\n",
    "        StructField(\"file_name\", StringType(), True)\n",
    "    ])\n",
    "    data = [\n",
    "        (1, \"User1\", \"user1@email.com\", \"INSERT\", base_timestamp, None, \"users.csv\"),\n",
    "        (2, \"User2\", \"user2@email.com\", \"INSERT\", base_timestamp, None, \"users.csv\"),\n",
    "    ]\n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "@notebook_fixture\n",
    "def sample_transactions_cdc(spark, base_timestamp):\n",
    "    \"\"\"Sample transactions CDC data for multi-table tests.\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", LongType(), False),\n",
    "        StructField(\"user_id\", LongType(), True),\n",
    "        StructField(\"amount\", StringType(), True),\n",
    "        StructField(\"operation\", StringType(), True),\n",
    "        StructField(\"operation_date\", TimestampType(), True),\n",
    "        StructField(\"_rescued_data\", StringType(), True),\n",
    "        StructField(\"file_name\", StringType(), True)\n",
    "    ])\n",
    "    data = [\n",
    "        (100, 1, \"99.99\", \"INSERT\", base_timestamp, None, \"transactions.csv\"),\n",
    "        (101, 2, \"149.99\", \"INSERT\", base_timestamp, None, \"transactions.csv\"),\n",
    "    ]\n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "conftest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
