{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a986994d-c339-4c45-bf9e-b2ea8e01a435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./conftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b6d857-9f67-4f3a-b18a-dfb7099e966a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dbx_test import NotebookTestFixture, run_notebook_tests\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, row_number, dense_rank, regexp_replace, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "from datetime import datetime, timedelta\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b42c39-f6d7-4ee0-bbe7-04514b477bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Helper Functions (Extracted from notebook for testing)\n",
    "# ============================================================================\n",
    "\n",
    "def deduplicate_cdc_data(df: DataFrame, id_col: str = \"id\", \n",
    "                          order_col: str = \"operation_date\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Deduplicate CDC data by keeping only the most recent record per ID.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with CDC data\n",
    "        id_col: Column to partition by for deduplication\n",
    "        order_col: Column to order by (descending) for selecting latest record\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with duplicates removed, keeping most recent record per ID\n",
    "    \"\"\"\n",
    "    windowSpec = Window.partitionBy(id_col).orderBy(col(order_col).desc())\n",
    "    return df.withColumn(\"rank\", row_number().over(windowSpec)).where(\"rank = 1\").drop(\"rank\")\n",
    "\n",
    "\n",
    "def apply_merge_logic(source_df: DataFrame, target_df: DataFrame, \n",
    "                      id_col: str = \"id\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply CDC merge logic to combine source and target DataFrames.\n",
    "    Simulates MERGE INTO behavior for testing purposes.\n",
    "    \n",
    "    Args:\n",
    "        source_df: Source DataFrame with CDC operations\n",
    "        target_df: Target DataFrame (current state)\n",
    "        id_col: Column to match on for merge\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame representing the merged result\n",
    "    \"\"\"\n",
    "    # Get columns for the result (excluding operation column)\n",
    "    result_columns = [c for c in target_df.columns if c != \"operation\"]\n",
    "    \n",
    "    # Handle deletes - remove matching IDs where operation is DELETE\n",
    "    delete_ids = (source_df\n",
    "                  .filter(col(\"operation\") == \"DELETE\")\n",
    "                  .select(id_col)\n",
    "                  .collect())\n",
    "    delete_id_list = [row[id_col] for row in delete_ids]\n",
    "    \n",
    "    # Filter out deleted records from target\n",
    "    target_filtered = target_df.filter(~col(id_col).isin(delete_id_list))\n",
    "    \n",
    "    # Handle updates and inserts\n",
    "    updates = source_df.filter(col(\"operation\") != \"DELETE\")\n",
    "    \n",
    "    # Remove matching IDs from target (will be replaced by updates)\n",
    "    update_ids = [row[id_col] for row in updates.select(id_col).collect()]\n",
    "    target_without_updates = target_filtered.filter(~col(id_col).isin(update_ids))\n",
    "    \n",
    "    # Combine: existing (non-updated) + updates\n",
    "    # Select only columns that exist in target\n",
    "    update_cols = [c for c in updates.columns if c in target_df.columns]\n",
    "    result = target_without_updates.union(updates.select(update_cols))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def process_cdf_for_gold(cdf_df: DataFrame, id_col: str = \"id\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Process CDF data for Gold layer, handling deduplication and filtering.\n",
    "    \n",
    "    Args:\n",
    "        cdf_df: DataFrame with CDF changes including _change_type and _commit_version\n",
    "        id_col: Column to deduplicate on\n",
    "        \n",
    "    Returns:\n",
    "        Deduplicated DataFrame ready for Gold layer merge\n",
    "    \"\"\"\n",
    "    windowSpec = Window.partitionBy(id_col).orderBy(col(\"_commit_version\").desc())\n",
    "    return (cdf_df\n",
    "            .withColumn(\"rank\", dense_rank().over(windowSpec))\n",
    "            .where(\"rank = 1 and _change_type != 'update_preimage'\")\n",
    "            .drop(\"_commit_version\", \"rank\"))\n",
    "\n",
    "\n",
    "def clean_address(df: DataFrame, address_col: str = \"address\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Clean address field by removing quotes.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        address_col: Name of address column to clean\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with cleaned address\n",
    "    \"\"\"\n",
    "    return df.withColumn(address_col, regexp_replace(col(address_col), \"\\\"\", \"\"))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Test Class: Deduplication Logic\n",
    "# ============================================================================\n",
    "\n",
    "class TestDeduplication(NotebookTestFixture):\n",
    "    \"\"\"Tests for CDC data deduplication functionality.\"\"\"\n",
    "    \n",
    "    @pytest.mark.parametrize(\"num_duplicates,expected_count\", [\n",
    "        (1, 1),   # Single record, no duplicates\n",
    "        (3, 1),   # Three records for same ID, expect 1\n",
    "        (5, 1),   # Five records for same ID, expect 1\n",
    "    ])\n",
    "    def test_deduplication_keeps_single_record(self,  cdc_schema, \n",
    "                                                base_timestamp, num_duplicates, \n",
    "                                                expected_count):\n",
    "        \"\"\"Test that deduplication keeps exactly one record per ID.\"\"\"\n",
    "        # Create test data with multiple records for same ID\n",
    "        data = [\n",
    "            (1, f\"Version{i}\", f\"Address{i}\", f\"email{i}@test.com\",\n",
    "             base_timestamp + timedelta(hours=i), \"UPDATE\", None, f\"file{i}.csv\")\n",
    "            for i in range(num_duplicates)\n",
    "        ]\n",
    "        df = spark.createDataFrame(data, cdc_schema)\n",
    "        \n",
    "        result = deduplicate_cdc_data(df)\n",
    "        \n",
    "        assert result.count() == expected_count\n",
    "    \n",
    "    def test_deduplication_keeps_latest_record(self,  cdc_schema, base_timestamp):\n",
    "        \"\"\"Test that deduplication keeps the most recent record.\"\"\"\n",
    "        data = [\n",
    "            (1, \"Old Name\", \"Old Address\", \"old@email.com\", base_timestamp, \"INSERT\", None, \"file1.csv\"),\n",
    "            (1, \"New Name\", \"New Address\", \"new@email.com\", \n",
    "             base_timestamp + timedelta(hours=2), \"UPDATE\", None, \"file2.csv\"),\n",
    "            (1, \"Middle Name\", \"Middle Address\", \"middle@email.com\", \n",
    "             base_timestamp + timedelta(hours=1), \"UPDATE\", None, \"file3.csv\"),\n",
    "        ]\n",
    "        df = spark.createDataFrame(data, cdc_schema)\n",
    "        \n",
    "        result = deduplicate_cdc_data(df)\n",
    "        row = result.collect()[0]\n",
    "        \n",
    "        assert row[\"name\"] == \"New Name\"\n",
    "        assert row[\"address\"] == \"New Address\"\n",
    "    \n",
    "    @pytest.mark.parametrize(\"ids,expected_counts\", [\n",
    "        ([1, 1, 2, 2, 3], {1: 1, 2: 1, 3: 1}),  # Multiple IDs with duplicates\n",
    "        ([1, 2, 3, 4, 5], {1: 1, 2: 1, 3: 1, 4: 1, 5: 1}),  # Unique IDs\n",
    "    ])\n",
    "    def test_deduplication_multiple_ids(self,  cdc_schema, base_timestamp, \n",
    "                                        ids, expected_counts):\n",
    "        \"\"\"Test deduplication works correctly across multiple IDs.\"\"\"\n",
    "        data = [\n",
    "            (id_val, f\"Name{i}\", f\"Address{i}\", f\"email{i}@test.com\",\n",
    "             base_timestamp + timedelta(hours=i), \"UPDATE\", None, f\"file{i}.csv\")\n",
    "            for i, id_val in enumerate(ids)\n",
    "        ]\n",
    "        df = spark.createDataFrame(data, cdc_schema)\n",
    "        \n",
    "        result = deduplicate_cdc_data(df)\n",
    "        \n",
    "        # Check count per ID\n",
    "        for id_val, expected in expected_counts.items():\n",
    "            actual = result.filter(col(\"id\") == id_val).count()\n",
    "            assert actual == expected, f\"ID {id_val}: expected {expected}, got {actual}\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Test Class: Merge Logic (Silver Layer)\n",
    "# ============================================================================\n",
    "\n",
    "class TestMergeLogic(NotebookTestFixture):\n",
    "    \"\"\"Tests for CDC merge/upsert logic in Silver layer.\"\"\"\n",
    "    \n",
    "    @pytest.mark.parametrize(\"operation,expected_exists\", [\n",
    "        (\"INSERT\", True),\n",
    "        (\"UPDATE\", True),\n",
    "        (\"DELETE\", False),\n",
    "    ])\n",
    "    def test_operation_types(self,  silver_schema, base_timestamp, \n",
    "                             operation, expected_exists):\n",
    "        \"\"\"Test that different CDC operations are handled correctly.\"\"\"\n",
    "        # Create target table\n",
    "        target_data = [(1, \"Original\", \"Orig Address\", \"orig@email.com\", \"INSERT\")]\n",
    "        target_df = spark.createDataFrame(target_data, silver_schema)\n",
    "        \n",
    "        # Create source with operation\n",
    "        source_schema = StructType([\n",
    "            StructField(\"id\", LongType(), False),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"email\", StringType(), True),\n",
    "            StructField(\"operation\", StringType(), True),\n",
    "        ])\n",
    "        source_data = [(1, \"Modified\", \"New Address\", \"new@email.com\", operation)]\n",
    "        source_df = spark.createDataFrame(source_data, source_schema)\n",
    "        \n",
    "        result = apply_merge_logic(source_df, target_df)\n",
    "        \n",
    "        id_exists = result.filter(col(\"id\") == 1).count() > 0\n",
    "        assert id_exists == expected_exists\n",
    "    \n",
    "    def test_insert_new_record(self,  silver_schema):\n",
    "        \"\"\"Test INSERT operation adds new record.\"\"\"\n",
    "        target_data = [(1, \"Existing\", \"Address1\", \"exist@email.com\", \"INSERT\")]\n",
    "        target_df = spark.createDataFrame(target_data, silver_schema)\n",
    "        \n",
    "        source_data = [(2, \"New User\", \"Address2\", \"new@email.com\", \"INSERT\")]\n",
    "        source_df = spark.createDataFrame(source_data, silver_schema)\n",
    "        \n",
    "        result = apply_merge_logic(source_df, target_df)\n",
    "        \n",
    "        assert result.count() == 2\n",
    "        assert result.filter(col(\"id\") == 2).count() == 1\n",
    "    \n",
    "    def test_update_modifies_existing(self,  silver_schema):\n",
    "        \"\"\"Test UPDATE operation modifies existing record.\"\"\"\n",
    "        target_data = [(1, \"Old Name\", \"Old Address\", \"old@email.com\", \"INSERT\")]\n",
    "        target_df = spark.createDataFrame(target_data, silver_schema)\n",
    "        \n",
    "        source_data = [(1, \"New Name\", \"New Address\", \"new@email.com\", \"UPDATE\")]\n",
    "        source_df = spark.createDataFrame(source_data, silver_schema)\n",
    "        \n",
    "        result = apply_merge_logic(source_df, target_df)\n",
    "        row = result.filter(col(\"id\") == 1).collect()[0]\n",
    "        \n",
    "        assert row[\"name\"] == \"New Name\"\n",
    "        assert row[\"address\"] == \"New Address\"\n",
    "    \n",
    "    def test_delete_removes_record(self,  silver_schema):\n",
    "        \"\"\"Test DELETE operation removes record.\"\"\"\n",
    "        target_data = [\n",
    "            (1, \"User1\", \"Address1\", \"user1@email.com\", \"INSERT\"),\n",
    "            (2, \"User2\", \"Address2\", \"user2@email.com\", \"INSERT\"),\n",
    "        ]\n",
    "        target_df = spark.createDataFrame(target_data, silver_schema)\n",
    "        \n",
    "        source_data = [(1, None, None, None, \"DELETE\")]\n",
    "        source_df = spark.createDataFrame(source_data, silver_schema)\n",
    "        \n",
    "        result = apply_merge_logic(source_df, target_df)\n",
    "        \n",
    "        assert result.count() == 1\n",
    "        assert result.filter(col(\"id\") == 1).count() == 0\n",
    "        assert result.filter(col(\"id\") == 2).count() == 1\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Test Class: CDF Processing (Gold Layer)\n",
    "# ============================================================================\n",
    "\n",
    "class TestCDFProcessing(NotebookTestFixture):\n",
    "    \"\"\"Tests for Change Data Feed processing for Gold layer.\"\"\"\n",
    "    \n",
    "    @pytest.mark.parametrize(\"change_types,expected_type\", [\n",
    "        ([\"insert\"], \"insert\"),\n",
    "        ([\"update_preimage\", \"update_postimage\"], \"update_postimage\"),\n",
    "        ([\"delete\"], \"delete\"),\n",
    "    ])\n",
    "    def test_cdf_change_type_filtering(self,  base_timestamp, \n",
    "                                        change_types, expected_type):\n",
    "        \"\"\"Test that correct change type is selected after deduplication.\"\"\"\n",
    "        schema = StructType([\n",
    "            StructField(\"id\", LongType(), False),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"_change_type\", StringType(), True),\n",
    "            StructField(\"_commit_version\", LongType(), True),\n",
    "        ])\n",
    "        \n",
    "        data = [(1, f\"Name_{ct}\", ct, i + 1) for i, ct in enumerate(change_types)]\n",
    "        df = spark.createDataFrame(data, schema)\n",
    "        \n",
    "        result = process_cdf_for_gold(df)\n",
    "        \n",
    "        if expected_type != \"update_preimage\":\n",
    "            assert result.count() == 1\n",
    "            row = result.collect()[0]\n",
    "            assert row[\"_change_type\"] == expected_type\n",
    "    \n",
    "    def test_cdf_filters_preimage(self):\n",
    "        \"\"\"Test that update_preimage records are filtered out.\"\"\"\n",
    "        schema = StructType([\n",
    "            StructField(\"id\", LongType(), False),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"_change_type\", StringType(), True),\n",
    "            StructField(\"_commit_version\", LongType(), True),\n",
    "        ])\n",
    "        \n",
    "        data = [\n",
    "            (1, \"Before Update\", \"update_preimage\", 2),\n",
    "            (1, \"After Update\", \"update_postimage\", 2),\n",
    "        ]\n",
    "        df = spark.createDataFrame(data, schema)\n",
    "        \n",
    "        result = process_cdf_for_gold(df)\n",
    "        \n",
    "        # Should only have postimage\n",
    "        assert result.count() == 1\n",
    "        assert result.collect()[0][\"name\"] == \"After Update\"\n",
    "    \n",
    "    def test_cdf_keeps_latest_version(self):\n",
    "        \"\"\"Test that only the latest commit version is kept.\"\"\"\n",
    "        schema = StructType([\n",
    "            StructField(\"id\", LongType(), False),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"_change_type\", StringType(), True),\n",
    "            StructField(\"_commit_version\", LongType(), True),\n",
    "        ])\n",
    "        \n",
    "        data = [\n",
    "            (1, \"Version 1\", \"insert\", 1),\n",
    "            (1, \"Version 2\", \"update_postimage\", 2),\n",
    "            (1, \"Version 3\", \"update_postimage\", 3),\n",
    "        ]\n",
    "        df = spark.createDataFrame(data, schema)\n",
    "        \n",
    "        result = process_cdf_for_gold(df)\n",
    "        \n",
    "        assert result.count() == 1\n",
    "        assert result.collect()[0][\"name\"] == \"Version 3\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Test Class: Data Cleaning\n",
    "# ============================================================================\n",
    "\n",
    "class TestDataCleaning(NotebookTestFixture):\n",
    "    \"\"\"Tests for data cleaning transformations.\"\"\"\n",
    "    \n",
    "    @pytest.mark.parametrize(\"input_address,expected_address\", [\n",
    "        ('\"123 Main St\"', '123 Main St'),\n",
    "        ('456 Oak Ave', '456 Oak Ave'),\n",
    "        ('\"City, \"State\" 12345\"', 'City, State 12345'),\n",
    "        ('No quotes here', 'No quotes here'),\n",
    "        ('\"\"', ''),\n",
    "    ])\n",
    "    def test_address_cleaning(self, input_address, expected_address):\n",
    "        \"\"\"Test that quotes are properly removed from addresses.\"\"\"\n",
    "        schema = StructType([\n",
    "            StructField(\"id\", LongType(), False),\n",
    "            StructField(\"address\", StringType(), True),\n",
    "        ])\n",
    "        df = spark.createDataFrame([(1, input_address)], schema)\n",
    "        \n",
    "        result = clean_address(df)\n",
    "        actual_address = result.collect()[0][\"address\"]\n",
    "        \n",
    "        assert actual_address == expected_address\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Test Class: End-to-End Pipeline Simulation\n",
    "# ============================================================================\n",
    "\n",
    "class TestEndToEndPipeline(NotebookTestFixture):\n",
    "    \"\"\"Integration tests simulating full CDC pipeline flow.\"\"\"\n",
    "    \n",
    "    def test_full_cdc_flow_insert_update_delete(self,  cdc_schema, \n",
    "                                                 silver_schema, base_timestamp):\n",
    "        \"\"\"Test complete CDC flow with INSERT, UPDATE, and DELETE operations.\"\"\"\n",
    "        # Initial inserts\n",
    "        initial_data = [\n",
    "            (1, \"Alice\", \"123 Main St\", \"alice@email.com\", base_timestamp, \"INSERT\", None, \"f1.csv\"),\n",
    "            (2, \"Bob\", \"456 Oak Ave\", \"bob@email.com\", base_timestamp, \"INSERT\", None, \"f1.csv\"),\n",
    "            (3, \"Charlie\", \"789 Pine Rd\", \"charlie@email.com\", base_timestamp, \"INSERT\", None, \"f1.csv\"),\n",
    "        ]\n",
    "        initial_df = spark.createDataFrame(initial_data, cdc_schema)\n",
    "        \n",
    "        # Deduplicate and prepare for merge\n",
    "        deduped = deduplicate_cdc_data(initial_df)\n",
    "        \n",
    "        # Create empty target\n",
    "        empty_target = spark.createDataFrame([], silver_schema)\n",
    "        \n",
    "        # Apply initial inserts\n",
    "        result = apply_merge_logic(\n",
    "            deduped.select(\"id\", \"name\", \"address\", \"email\", \"operation\"),\n",
    "            empty_target\n",
    "        )\n",
    "        \n",
    "        assert result.count() == 3\n",
    "        \n",
    "        # Apply updates\n",
    "        update_data = [\n",
    "            (1, \"Alice Updated\", \"999 New St\", \"alice.new@email.com\", \n",
    "             base_timestamp + timedelta(hours=1), \"UPDATE\", None, \"f2.csv\"),\n",
    "        ]\n",
    "        update_df = spark.createDataFrame(update_data, cdc_schema)\n",
    "        deduped_update = deduplicate_cdc_data(update_df)\n",
    "        \n",
    "        result = apply_merge_logic(\n",
    "            deduped_update.select(\"id\", \"name\", \"address\", \"email\", \"operation\"),\n",
    "            result\n",
    "        )\n",
    "        \n",
    "        alice_row = result.filter(col(\"id\") == 1).collect()[0]\n",
    "        assert alice_row[\"name\"] == \"Alice Updated\"\n",
    "        assert result.count() == 3\n",
    "        \n",
    "        # Apply delete\n",
    "        delete_data = [\n",
    "            (2, None, None, None, base_timestamp + timedelta(hours=2), \"DELETE\", None, \"f3.csv\"),\n",
    "        ]\n",
    "        delete_df = spark.createDataFrame(delete_data, cdc_schema)\n",
    "        deduped_delete = deduplicate_cdc_data(delete_df)\n",
    "        \n",
    "        result = apply_merge_logic(\n",
    "            deduped_delete.select(\"id\", \"name\", \"address\", \"email\", \"operation\"),\n",
    "            result\n",
    "        )\n",
    "        \n",
    "        assert result.count() == 2\n",
    "        assert result.filter(col(\"id\") == 2).count() == 0\n",
    "    \n",
    "    @pytest.mark.parametrize(\"batch_size\", [1, 5, 10])\n",
    "    def test_multiple_batches(self,  cdc_schema, silver_schema, \n",
    "                              base_timestamp, batch_size):\n",
    "        \"\"\"Test processing multiple CDC batches of varying sizes.\"\"\"\n",
    "        result = spark.createDataFrame([], silver_schema)\n",
    "        \n",
    "        for batch_num in range(3):\n",
    "            batch_data = [\n",
    "                (batch_num * batch_size + i, \n",
    "                 f\"User_{batch_num}_{i}\", \n",
    "                 f\"Address_{batch_num}_{i}\",\n",
    "                 f\"user{batch_num}_{i}@email.com\",\n",
    "                 base_timestamp + timedelta(hours=batch_num),\n",
    "                 \"INSERT\", None, f\"file{batch_num}.csv\")\n",
    "                for i in range(batch_size)\n",
    "            ]\n",
    "            batch_df = spark.createDataFrame(batch_data, cdc_schema)\n",
    "            deduped = deduplicate_cdc_data(batch_df)\n",
    "            \n",
    "            result = apply_merge_logic(\n",
    "                deduped.select(\"id\", \"name\", \"address\", \"email\", \"operation\"),\n",
    "                result\n",
    "            )\n",
    "        \n",
    "        expected_count = batch_size * 3\n",
    "        assert result.count() == expected_count\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Test Class: Schema Handling\n",
    "# ============================================================================\n",
    "\n",
    "class TestSchemaHandling(NotebookTestFixture):\n",
    "    \"\"\"Tests for schema validation and handling.\"\"\"\n",
    "    \n",
    "    def test_null_handling_in_delete(self,  cdc_schema, base_timestamp):\n",
    "        \"\"\"Test that NULL values in DELETE operations are handled correctly.\"\"\"\n",
    "        data = [\n",
    "            (1, None, None, None, base_timestamp, \"DELETE\", None, \"file.csv\"),\n",
    "        ]\n",
    "        df = spark.createDataFrame(data, cdc_schema)\n",
    "        \n",
    "        result = deduplicate_cdc_data(df)\n",
    "        \n",
    "        assert result.count() == 1\n",
    "        row = result.collect()[0]\n",
    "        assert row[\"id\"] == 1\n",
    "        assert row[\"operation\"] == \"DELETE\"\n",
    "    \n",
    "    @pytest.mark.parametrize(\"rescued_data\", [\n",
    "        None,\n",
    "        '{\"extra_field\": \"value\"}',\n",
    "        '',\n",
    "    ])\n",
    "    def test_rescued_data_handling(self,  cdc_schema, base_timestamp, rescued_data):\n",
    "        \"\"\"Test that _rescued_data column is handled correctly.\"\"\"\n",
    "        data = [\n",
    "            (1, \"Name\", \"Address\", \"email@test.com\", base_timestamp, \"INSERT\", rescued_data, \"file.csv\"),\n",
    "        ]\n",
    "        df = spark.createDataFrame(data, cdc_schema)\n",
    "        \n",
    "        result = deduplicate_cdc_data(df)\n",
    "        \n",
    "        assert result.count() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf87095-1af2-402f-b25b-2bc0df7a00e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dbx_test.fixtures import register_fixture, notebook_fixture\n",
    "\n",
    "register_fixture('base_timestamp', datetime(2024, 1, 1, 10, 0, 0))\n",
    "\n",
    "@notebook_fixture\n",
    "def base_timestamp():\n",
    "    \"\"\"Base timestamp for test data.\"\"\"\n",
    "    return datetime(2024, 1, 1, 10, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c0a11d-5bd2-4a7d-80c2-c24bf4900f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_notebook_tests()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dbx_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
